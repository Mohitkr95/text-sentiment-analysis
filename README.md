# Advanced Sentiment Analysis System

A comprehensive, production-ready sentiment analysis system with advanced text preprocessing, ensemble machine learning models, and detailed evaluation capabilities.

## üöÄ Features

### Advanced Text Preprocessing
- **Smart Cleaning**: URL removal, HTML tag cleaning, contraction expansion
- **Lemmatization**: Advanced word normalization using WordNet
- **Sentiment-Aware Stop Words**: Preserves negation words crucial for sentiment
- **Statistical Analysis**: Comprehensive text statistics and quality metrics

### Machine Learning Models
- **Multiple Algorithms**: Logistic Regression, Random Forest, SVM, Naive Bayes, KNN
- **Ensemble Methods**: Hard voting, soft voting classifiers
- **Class Imbalance Handling**: SMOTE oversampling, balanced classifiers
- **Hyperparameter Tuning**: Grid search with cross-validation
- **Feature Selection**: Chi-square based feature selection

### Feature Engineering
- **TF-IDF Vectorization**: Advanced n-gram analysis (1-2 grams)
- **Additional Features**: Text length, punctuation counts, sentiment indicators
- **Smart Filtering**: Min/max document frequency filtering

### Comprehensive Evaluation
- **Multiple Metrics**: Accuracy, Precision, Recall, F1-score, ROC-AUC
- **Visualizations**: Confusion matrices, ROC curves, model comparisons
- **Interactive Dashboard**: Plotly-based interactive evaluation
- **Cross-Validation**: Robust performance estimation

### Production Features
- **Logging**: Comprehensive logging with timestamps and structured output
- **Model Persistence**: Save/load trained models and preprocessors
- **Batch Prediction**: Process multiple texts efficiently
- **CLI Interface**: Command-line tools for training and prediction

## üõ†Ô∏è Installation

1. **Clone the repository**:
```bash
git clone <repository-url>
cd text-sentiment-analysis
```

2. **Create a virtual environment** (recommended):
```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Download NLTK data** (automatic on first run):
```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('averaged_perceptron_tagger')"
```

## üéØ Usage

### Training the Model

Run the complete training pipeline:

```bash
python main.py
```

This will:
1. Load and explore the dataset
2. Preprocess text with advanced cleaning
3. Extract TF-IDF and additional features
4. Train multiple ML models with hyperparameter tuning
5. Create ensemble models
6. Evaluate performance with comprehensive metrics
7. Save the best model and generate reports

### Making Predictions

#### Single Text Prediction
```bash
python predict.py "This restaurant has amazing food and great service!"
```

#### Batch Prediction from File
```bash
python predict.py --file reviews.txt
```

#### Verbose Output
```bash
python predict.py "Great experience!" --verbose
```

### Configuration

Modify `config/config.py` to adjust:
- Model hyperparameters
- Feature extraction settings
- Data paths
- Logging preferences

## üìä Model Performance

The system typically achieves:
- **Accuracy**: 85-90%
- **F1-Score**: 0.85-0.90
- **ROC-AUC**: 0.90-0.95

Performance improvements over basic approach:
- **+15-20%** accuracy improvement
- **Better generalization** through ensemble methods
- **Robust preprocessing** handles real-world text better
- **Class imbalance** handling improves minority class prediction

## üîß Advanced Features

### Text Preprocessing Improvements
```python
# Handles contractions
"don't" ‚Üí "do not"
"won't" ‚Üí "will not"

# Preserves sentiment-critical words
"not good" ‚Üí keeps "not" (removed from typical stop words)

# Advanced cleaning
URLs, HTML tags, special characters removed
Lemmatization: "running" ‚Üí "run"
```

### Feature Engineering
- **TF-IDF with bigrams**: Captures phrase-level sentiment
- **Text statistics**: Length, punctuation, capitalization
- **Sentiment indicators**: Negation counts, intensifiers
- **Feature selection**: Removes noisy features

### Ensemble Methods
- **Voting classifiers**: Combine predictions from multiple models
- **Model diversity**: Different algorithms capture different patterns
- **Hyperparameter optimization**: Each model tuned individually

## üìà Logging and Monitoring

Comprehensive logging includes:
- Training progress and timing
- Model performance metrics
- Feature extraction statistics
- Error handling and debugging info
- Saved to `logs/` directory with timestamps

## üé® Visualizations

The system generates:
- **Confusion matrices**: True vs predicted classifications
- **ROC curves**: Model discrimination ability
- **Model comparison charts**: Performance across algorithms
- **Interactive dashboard**: Plotly-based exploration tool

## üîç Example Output

```
=== FINAL RESULTS SUMMARY ===
Best Model: soft_voting
Best Validation F1 Score: 0.8756
Test F1 Score: 0.8643
Test Accuracy: 0.8700
Cross-validation F1: 0.8612 ¬± 0.0156
Test ROC AUC: 0.9234
Total Features Used: 3010
```

## üöÄ Performance Optimizations

- **Parallel processing**: Multi-core model training
- **Memory efficient**: Sparse matrix operations
- **Feature selection**: Reduces dimensionality
- **Caching**: Preprocessed data and models saved
- **Batch processing**: Efficient prediction on multiple texts

## üõ°Ô∏è Error Handling

- Graceful handling of missing/malformed data
- Robust text preprocessing with fallbacks
- Model loading validation
- Comprehensive error logging

## üìù Future Improvements

Potential enhancements:
- **Deep learning models**: BERT, LSTM, Transformer-based models
- **Real-time API**: Flask/FastAPI web service
- **Model monitoring**: Drift detection and retraining
- **A/B testing**: Compare model versions
- **Multi-language support**: Extend beyond English

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.